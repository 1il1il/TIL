# [인공지능 및 기계학습 개론Ⅰ](https://kaist.edwith.org/machinelearning1_17#)

##  Week 1

### Motivation and Basics

### Maximum Likelihood Estimation (MLE)

- 로그를 취해 식을 맵핑

### Simple Error Bound

- 우리는 확률을 추정한 것
- 추정한 확률과 실제 확률은 차이가 있을 수 있다
- Error Bound 커지면 Error이 발생할 확률이 작아짐
- 횟수가 커지면 Error이 발생할 확률이 작아짐

### Probably Approximate Correct (PAC) Learning

### Incorporating Prior Knowledge

- Posterior = Likelihood x Prior Knowledge / Normalizing Constant
- 데이터가 주어졌을때 해당 추정(확률)이 나올 확률
- Beta distribution

### Maximum a Posteriori Estimation (MAP)

- 시행횟수가 작으면 MLE와 MAP는 차이가 생김
- 시행횟수가 많으면 차이가 줄어듦

### Probability Distribution

- A function mapping an evnet to a probability
- Normal Distribution
- Beta Distribution
  - 범위가 0-1로 지정 (확률과 잘 맞는다)
- Binomial Distribution
  - 이산확률분포
- Multinomial Distribution

## Week 2

### Rule Based Learning

- FInd-S Algorithm
  - 사건이 발생했을때의 조건을 분석하여 가설 생성
- Version Space
  - Possible hypotheses == Version Space. VS
  - 여러 hypotheses의 집합
- Candidate Elimination Algorithm
  - S is Maximally Specific h in H
  - G is Maximally general h in H
    - S와 G를 조금씩 고쳐나가기
    - 그렇게 만든 S와 G 사이에 true function c가 있을 것이다.

### Decision Tree

- 하나의 기준으로 분기하는 가지
- 어떤 기준으로 선택해야하는가
  - Entropy
    - 불확실성을 감소시키기
  - Conditional Entropy
    - Infomation Gain
- Top-Down Induction Algorithm
- 현재의 데이터에 최적화
- 앞으로 들어올 데이터에 대해선 불확실함

### Liner Regression

- 오차를 최소화하는 식을 찾기

## Week 3

### Optimal classification 

- error minimization
- f* = argminf p(f(x)=/=Y)
  - f(x)가 Y가 아닐 확률을 최소화하는 f를 찾기
- p(Y=y|X=x) = p(X=x|Y=y) p(Y=y)
  - p(X=x|Y=y): Class conditional Density, Likelihood
  - p(Y=y): Class Prior, Prior

### Naive Bayes Classifier

- p(X=x|Y=y)에서 x가 많을 경우 무수히 많은 경우의 수를 따져봐야함
  - (2^n-1)k개의 경우의 수 따져야함, n: x의 갯수, k: y의 갯수
- Conditional Independence
  - p(X=x|Y=y) > Πi p(Xi=xi|Y=y)
  - p(x1|x2,y) = p(x1|y)
- Conditional vs. Marginal Independence
  - Marginal Independence
    - p(x)=p(x|y)인 경우에면 Independent
  - Conditional Independence
    - p(x1|x2,y)=p(x1|y)
    - (2-1)dk개의 경우의 수로 줄어듦
- Problem
  - Naive assumption
  - Incorrect Probability Estimations
    - MLE with insufficient data
    - MAP with stupid prior

## Week 4

### Logistic Regression

- Linear Function vs. Non-Linear Function
- Sigmoid function
  - Bounded
  - Differentiable
  - Real Function
  - Defined for all real inputs
  - With positive derivative
- Logistic Function, Logit Function
  - Logistic Function: f(x) = 1/1+e^-x
  - Logit Function: f(x) = log(x/1-x)
  - Logistic Function의 역함수는 Logit Function
- Maximum Conditional Likelihood Estimation (MCLE)

### Gradient descent algorithm

- Example function: Rosenbrock function
  - f(x1, x2) = (1 - x1)^2+100(x2 - x1^2)^2

### Logistic Regression Classifier

- Gaussian Naive Bayes
- Derivation from the Naive Bayes to the logistic regression
  - 두 분산이 같다는 가정하에
- Naive Bayes에선 prior 정보를 추가할 수 있음
- Logistic Regression은 좀 더 간결함

## Generative - Discriminative pair

- Generative model: p(Y|X)=p(X,Y)/p(X)=p(X|Y)p(Y)/p(X)
  - Bayesian, Prior, Modeling the joint probability
  - Naive Bayes Classifier
- Discriminative model: p(Y|X)
  - Estimate the parameters of p(Y|X) from the data
  - Modeling th conditional probaility
  - Logistic Regression 

## Week 5

### Support Vector Mashine

- Decision Boundary without Prob.

  - 점들과 거리가 가장 먼 선 찾기
  - Decision Boundary Line
    - wx + b = 0
      - wx + b > 0: Pos
      - wx + b < 0: Neg
  - Confidence level
    - (wxi + b)yi: 항상 양수가 나온다
      - Pos: +1, Neg: -1로 설정(yi)
  - Margin
    - Decision Boundary에서 가장 가까운 점까지의 거리
      - Decision Boundary위의 점 xp, Pos 영역의 임의의 점 x
      - x = xp + r(w/||w||)
      - f(x) = wx + b = w(xp + r(w/||w||)) + b = wxp +b + r(ww/||w||) =  r(ww/||w||) = r||w||, (f(xp)=0)
      - r = f(x)/||w||, (f(x)=a)
    - Optimization problem
      - Max 2r = 2a/||w||
      - s.t. (wxj + b)yj >= a, j: learing instance
      - a can be normalized
        - min ||w||
        - s.t. (wxj + b)yj >= 1
      - This become a quadratic optimaization
        - ||w|| = √(w1^2 + w2^2)

- Support Vector Mashine with Hard Margin

  - No error cases are allowed
  - 현실에선 이렇게 하기 힘들다

  - 해결 방안 1: Non-linear Decision Boundary
  - 해결 방안 2: Error를 허용하지만 일정 페널티 부여
    1. 0-1 Loss: Decision Boundary를 넘어가면 패널티 부터
       - Decision Boundary에서 얼마나 떨어졌는지 상관없이 모두 같은 패널티가 부여됨
       - 이를 반영한 최적화 문제를 풀기 어렵다
    2. Hinge loss: 거리가 멀어질수록 페널티가 증가
       - slack variable > 1 when mis-classified
       - min ||W|| + CΣslack variable
         - s.t. (wxj + b)yj >= 1 - slack variable, slack variable >= 0
         - C = trade-off parameter
       - 이 모델을 Soft-Margin SVM이라고 함

- Soft-Margin SVM

  - Soften the constraints by adding slack variable
  - C가 커지면 잘못 분류된 값에 대한 페널티가 커짐

- Kernel trick

  - SVM turns
    - Classification > Constrained quadratic programming
  - Lagrange method
    - Lagrange  Prime Function
    - Lagrange Multiplier
    - Lagrange  Dual Function
  - Primal and Dual Problem
    - Strong duality
      - Primal Solution = Dual Solution, When KKT condition are satisfied
  - Dual Problem of SVM

- Mapping Functions

  - 선형으로 분리 불가능한 문제를 차원을 확장하여 분리 가능으로 변환
  - 하지만 차원이 점점 커진다는 단점이 존재

- Kernel Function

  - The Kernel calculates the inner product of two vectorsin different space
  - Polynomial Kernel Function

- Dual SVM with Kernel Trick

  - Sign(wψ(x)+b)를 이용해서 분류
  - 정확한 값을 계산하는건 어렵다
  - Easy expand to the high dimension features
