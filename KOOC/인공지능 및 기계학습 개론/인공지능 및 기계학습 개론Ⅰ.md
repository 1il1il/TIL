# [인공지능 및 기계학습 개론Ⅰ](https://kaist.edwith.org/machinelearning1_17#)

##  Week 1

### Motivation and Basics

### Maximum Likelihood Estimation (MLE)

- 로그를 취해 식을 맵핑

### Simple Error Bound

- 우리는 확률을 추정한 것
- 추정한 확률과 실제 확률은 차이가 있을 수 있다
- Error Bound 커지면 Error이 발생할 확률이 작아짐
- 횟수가 커지면 Error이 발생할 확률이 작아짐

### Probably Approximate Correct (PAC) Learning

### Incorporating Prior Knowledge

- Posterior = Likelihood x Prior Knowledge / Normalizing Constant
- 데이터가 주어졌을때 해당 추정(확률)이 나올 확률
- Beta distribution

### Maximum a Posteriori Estimation (MAP)

- 시행횟수가 작으면 MLE와 MAP는 차이가 생김
- 시행횟수가 많으면 차이가 줄어듦

### Probability Distribution

- A function mapping an evnet to a probability
- Normal Distribution
- Beta Distribution
  - 범위가 0-1로 지정 (확률과 잘 맞는다)
- Binomial Distribution
  - 이산확률분포
- Multinomial Distribution

## Week 2

### Rule Based Learning

- FInd-S Algorithm
  - 사건이 발생했을때의 조건을 분석하여 가설 생성
- Version Space
  - Possible hypotheses == Version Space. VS
  - 여러 hypotheses의 집합
- Candidate Elimination Algorithm
  - S is Maximally Specific h in H
  - G is Maximally general h in H
    - S와 G를 조금씩 고쳐나가기
    - 그렇게 만든 S와 G 사이에 true function c가 있을 것이다.

### Decision Tree

- 하나의 기준으로 분기하는 가지
- 어떤 기준으로 선택해야하는가
  - Entropy
    - 불확실성을 감소시키기
  - Conditional Entropy
    - Infomation Gain
- Top-Down Induction Algorithm
- 현재의 데이터에 최적화
- 앞으로 들어올 데이터에 대해선 불확실함

### Liner Regression

- 오차를 최소화하는 식을 찾기

