## [인공지능 및 기계학습 심화](https://kaist.edwith.org/aiml-adv#)

## Week 1

### Dirichlet Process: Gaussian Mixture Model and Dirichlet Distribution Review

- Gaussian Mixture Model
- Dirichlet Distribution
  - GMM의 z: multinomial dist의 choice size varying(parameter 생성 방법) > Dirichlet dist 사용

### Dirichlet Process: Multinomial-Dirichlet Conjugate Relation

- Multinomial distribution
  - P(D|θ)
- Dirichlet distribution
  - P(θ|a)
- Bayesian Posterior
  - P(θ|D, a) ∝ P(D|θ) P(θ|a)
- Coming back to the Dirichlet distribution : Conjugate Prior
  - The likeilhood of the Dirichlet distribution is the conjugate prior of the multinomial distribution
- Dirichlet distribution with D as a single observation with i-th choice
  - θ|a ~ Dir(a1, ..., ai, ..., an)
  - θ|a, D ~ Dir(a1, ..., ai+1, ..., an)
  - 데이터가 관측되어 파라메터가 변화하게되었다

### Dirichlet Process: Definition

- Dirichlet Process, G|a,H ~ DP(a, H)

  - (G(A1), ..., G(Ar)) | a, H ~ Dir(aH(A1), ..., aH(Ar))
    - A1 ∩ ... ∩ Ar = 공집합,  A1 ∪ ... ∪ Ar = 전체집합
    - Dirichlet distribution의 parameter에 PDF 확률 분포를 가미
    - 이 결과(G(A1), ..., G(Ar))는 Multinomial distribution의 parameter를 만들어 내는 것, 선택지의 확률, Prior
  - Properties
    - E[G(A)] = H(A)
    - V[G(A)] = H(A)(1-H(A)) / a + 1
    - H : Base distribution
    - a : Concentration parameter, strength parameter (strength of prior)
- Posterior distribution given a dataset of θ1 ... θn

  - Posterior ∝ Likelihood X Prior
  - Multinomial-Dirichlet conjugate eelationship
    - The posterior become the Dirichlet distribution, again, adjusted to reflect the likelihood
  - (G(A1), ..., G(Ar)) | θ1 ... θn, a, H ~ Dir(aH(A1), ..., aH(Ar)+ nr)
    - nk = |{θi|θi ∈ Ak, 1 ≤ i ≤ n}|
      - θi:특정한 영역에 속하는지 속하지 않는지
    - n번의 trial이 있고 이때마다 특정한 선택지를 선택, 이벤트가 발생
- Sampling from Dirichlet Process
  - Multiple generation schemes, or construction, exist
    - Stick Breaking Scheme
    - Polya Urn Scheme
    - Chinese Restaurant Process Scheme

### Dirichlet Process: Stick-Breaking Construction

- Image that we create a probability mass function on infinite choice
  - k = 1, 2, ... ∞
  - vk | a ~ Beta(1, a): k번째 선택지 (1: fix, a:prior)
  - Bk = vk∏k-1(1-vl): 선택지가 선택될 확률
    - First Stick Length = 1
      - B1 - v1
    - Second Stick Length = 1 - v1
      - B2 - v2(1-v1)
    - 3rd Stick Length = ∏2(1-vl)
- Common notation is
  - B ~ GEM(a)
- We were constructing a distribution for the Dirichlet process
  - G|a, H ~ DP(a, H)
  - θk chooses a n-th broken stick, and the stick length is the prob
  - We know the existence of the infinite-th stick lenght

### Dirichlet Process: Polya Urn Scheme

- This enables sampling an observation from the Dirichlet process without constructing G|a, H ~ DP(a, H)
- Stick-breaking(distribution) construction vs. Polya Urn sampling from distribution
- Polya Urn Scheme
  - Create an empty urn
  - Do
    - toss = Coin toss from [0, a + n - 1]
    - If 0 ≤ toss < a
      - Add a ball to the urn by paining the ball as a sample from θn ~ H
    - If a ≤ toss < a + n - 1
      - Pick a ball from the urm
      - Return the ball and a new ball with the same color to the urn
    - a가 크면 새로운 공이 더 많이 들어옴

### Dirichlet Process: Chinese Restaurant Process

- Chinese Restaurant Process
  - Assume Infinite number of tables in a restaurant
  - First customer sits at the first table
  - Loop for Customer N sits at:
    - Table k with P(θn|θ1 ... θn-1, a) = nk / a + n - 1
    - A new table k + 1 with P(0n|01 ... 0n-1, a) = a / a + n - 1
  - Properties of Chinese restaurant process
    - Clustering formation
    - Rich-get-richer property
    - No fixed number of clusters with a fixed number of instances
    - Almost identical to Polya Urn Scheme

### Dirichlet Process: Random Process Review

- Random process, a.k.a. stochastic prosess, is
  - An infinite indexed collection of random variables, {X(t)|t ∈ T}
    - Index parameter: t
      - Can be time, space...
    - A function, X(t, ω), where t ∈ T and ω ∈ Ω
      - Outcome of the underlying random experiment: ω (Observation)
      - Fixed t > X(t, ω) is a random variable over Ω
        - 특정 지점에서의 샘플링 결과 > Prior distribution
      - Fixed ω > X(t, ω) is a deterministic function of t, a sample function
        - 어떤 사건 관점에서 생각, 여러번의 샘플링의 결과
- Example of random process
  - Gaussian process
    - Fixed t, a random variable following a Gaussian distribution
    - Fixed ω, a deterministic curve of t
  - Dirichlet Process
    - Fixed t, a random variable following a Dirichlet distribution
    - Fixed ω, a deterministic placement over clusters

### Dirichlet Process: De Finetti's Theorem

- Exchangeability
  - A joint probability distribution is exchangeable if it is invariant to permutation
  - Given a permutation of S
  - P(x1, x2,..., xn) = P(xs(1), xs(2),..., xs(n))
- (De Finetti, 1935) If (x1, x2, ...) are infinitely exchangeable, then the joint probability P(x1, x2,..., xn)  has a representation as a mixture
  - P(x1, x2,..., xn) = ∫(∏P(xi|θ)) dP(θ) = ∫P(θ) (∏P(xi|θ)) dθ
  - For some random variable θ
    - Independent and identically distributed > Exchangeable 
    - Exchangeable > IID: No. A counter example is the Polya urn sampling
  - Chinese restaurant process is an exchangeable process
    - Why is exchangeability important?
      - Enables a simple derivation of Gibbs sampler for the inference
      - We remove the instance of the next Gibbs sampling from the existing cluster assignment

### Dirichlet Process: GMM Review

- P(x) = ∑ P(zk) P(x|z) = ∑ πk N(x|μk, ∑k)
  - πk: Mixing coeffiecnt
  - N(x|μk, ∑k): component (k: number of component)
- How to model such mixture?
  - Mixing coeffiecnt, or Selection variable: zk
    - π는 파라메터, π의 갯수가 k, 이것이 z, μ, ∑에 영향을 준다
      - π를 Bayesian으로 만들기, 단순 파라메터(MLE 형태)가 아니라 어떤 distribution에서 sampling되는 형태로 만들어야한다
      - Sampling distribution이 Nonparametric
        - 이래야 k를 풀수 있다

