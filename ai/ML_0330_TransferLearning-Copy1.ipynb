{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-morgan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-denmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = EfficientNetB3(weights='imagenet',\n",
    "                   include_top=False,\n",
    "                   input_shape=(150,150,3))\n",
    "model_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-environment",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../data/cat_dog_full'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1/255)\n",
    "batch_size = 20\n",
    "\n",
    "def extract_feature(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count,4,4,512))\n",
    "    labels = np.zeros(shape=(sample_count,))\n",
    "    \n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=(150,150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for x_data_batch, t_data_batch in generator:\n",
    "        feature_batch = model_base.predict(x_data_batch)\n",
    "        features[i*batch_size:(i+1)*batch_size] = feature_batch\n",
    "        labels[i*batch_size:(i+1)*batch_size] = t_data_batch\n",
    "        i += 1\n",
    "        \n",
    "        if i*batch_size >= sample_count:\n",
    "            break\n",
    "            \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = extract_feature(train_dir, 14000)\n",
    "validation_features, validation_labels = extract_feature(validation_dir, 6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-turtle",
   "metadata": {},
   "source": [
    "- Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.reshape(train_features, (14000, 4*4*512))\n",
    "validation_features = np.reshape(validation_features, (6000, 4*4*512))\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, activation='relu',\n",
    "                input_shape=(4*4*512,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=RMSprop(learning_rate=2e-5), loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-chest",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_features, train_labels, epochs=30,\n",
    "                    validation_data=(validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc =  history.history['val_accuracy']\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss =  history.history['val_loss']\n",
    "\n",
    "plt.plot(train_loss, color='b', label='training loss')\n",
    "plt.plot(val_loss, color='r', label='validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-liquid",
   "metadata": {},
   "source": [
    "- 조금 더 나은 결과를 얻으려면 데이터가 많아져야 할것 같음\n",
    "- 증식을 포함하여 다시 작성\n",
    "- pretrained network와 우리 classifier를 합쳐서 모델을 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-ridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-pickup",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../data/cat_dog_full'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1/255,    \n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, \n",
    "    target_size=(150,150),\n",
    "    batch_size=20,\n",
    "    classes=['cats','dogs'],\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir, \n",
    "    target_size=(150,150),\n",
    "    batch_size=20,\n",
    "    classes=['cats','dogs'],\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# pretrained network\n",
    "model_base = VGG16(include_top=False, weights='imagenet', input_shape=(150,150,3))\n",
    "# model_base의 weight학습을 동결\n",
    "model_base.trainable=False\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# pretrained network를 우리의 모델 앞에 추가\n",
    "model.add(model_base)\n",
    "\n",
    "model.add(Flatten(input_shape=(4*4*512,)))\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(learning_rate=2e-5), loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-bleeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator, steps_per_epoch=700, epochs=30,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-wayne",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "periodic-fishing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnetb3 (Functional)  (None, 5, 5, 1536)        10783535  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 38400)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               9830656   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 20,614,448\n",
      "Trainable params: 9,830,913\n",
      "Non-trainable params: 10,783,535\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "100/100 [==============================] - 15s 147ms/step - loss: 0.3589 - accuracy: 0.8325 - val_loss: 0.0964 - val_accuracy: 0.9720\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.2391 - accuracy: 0.9030 - val_loss: 0.0748 - val_accuracy: 0.9780\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.2146 - accuracy: 0.9080 - val_loss: 0.0700 - val_accuracy: 0.9780\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 13s 133ms/step - loss: 0.2101 - accuracy: 0.9180 - val_loss: 0.0793 - val_accuracy: 0.9740\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.1829 - accuracy: 0.9250 - val_loss: 0.0697 - val_accuracy: 0.9780\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 13s 135ms/step - loss: 0.1816 - accuracy: 0.9285 - val_loss: 0.0823 - val_accuracy: 0.9750\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.1757 - accuracy: 0.9310 - val_loss: 0.0688 - val_accuracy: 0.9780\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 13s 133ms/step - loss: 0.1502 - accuracy: 0.9420 - val_loss: 0.0696 - val_accuracy: 0.9750\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.1546 - accuracy: 0.9445 - val_loss: 0.0708 - val_accuracy: 0.9740\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.1564 - accuracy: 0.9455 - val_loss: 0.0702 - val_accuracy: 0.9750\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 13s 133ms/step - loss: 0.1386 - accuracy: 0.9505 - val_loss: 0.0764 - val_accuracy: 0.9730\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.1511 - accuracy: 0.9390 - val_loss: 0.0811 - val_accuracy: 0.9750\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.1321 - accuracy: 0.9515 - val_loss: 0.0703 - val_accuracy: 0.9750\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.1337 - accuracy: 0.9495 - val_loss: 0.0749 - val_accuracy: 0.9770\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.1418 - accuracy: 0.9415 - val_loss: 0.0742 - val_accuracy: 0.9740\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.1223 - accuracy: 0.9575 - val_loss: 0.0755 - val_accuracy: 0.9790\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.1033 - accuracy: 0.9575 - val_loss: 0.0783 - val_accuracy: 0.9780\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 13s 133ms/step - loss: 0.1022 - accuracy: 0.9615 - val_loss: 0.0752 - val_accuracy: 0.9800\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.1251 - accuracy: 0.9520 - val_loss: 0.0747 - val_accuracy: 0.9790\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.1310 - accuracy: 0.9500 - val_loss: 0.0772 - val_accuracy: 0.9780\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "\n",
    "\n",
    "base_dir = '../data/cat_dog_small'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator()\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, \n",
    "    target_size=(150,150),\n",
    "    batch_size=20,\n",
    "    classes=['cats','dogs'],\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir, \n",
    "    target_size=(150,150),\n",
    "    batch_size=20,\n",
    "    classes=['cats','dogs'],\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# pretrained network\n",
    "model_base = EfficientNetB3(include_top=False, weights='imagenet', input_shape=(150,150,3))\n",
    "# model_base의 weight학습을 동결\n",
    "model_base.trainable=False\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# pretrained network를 우리의 모델 앞에 추가\n",
    "model.add(model_base)\n",
    "\n",
    "model.add(Flatten(input_shape=(4*4*512,)))\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(learning_rate=2e-5), loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_generator, steps_per_epoch=100, epochs=20,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-waters",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alpha-virus",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 15s 152ms/step - loss: 0.1543 - accuracy: 0.9490 - val_loss: 0.0779 - val_accuracy: 0.9770\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.1186 - accuracy: 0.9575 - val_loss: 0.0788 - val_accuracy: 0.9770\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.1152 - accuracy: 0.9565 - val_loss: 0.0799 - val_accuracy: 0.9760\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.1219 - accuracy: 0.9610 - val_loss: 0.0792 - val_accuracy: 0.9760\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 13s 135ms/step - loss: 0.1205 - accuracy: 0.9535 - val_loss: 0.0746 - val_accuracy: 0.9810\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 13s 135ms/step - loss: 0.1149 - accuracy: 0.9605 - val_loss: 0.0789 - val_accuracy: 0.9780\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 14s 141ms/step - loss: 0.0979 - accuracy: 0.9605 - val_loss: 0.0789 - val_accuracy: 0.9790\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.1064 - accuracy: 0.9575 - val_loss: 0.0800 - val_accuracy: 0.9790\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.1120 - accuracy: 0.9675 - val_loss: 0.0860 - val_accuracy: 0.9770\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.1067 - accuracy: 0.9610 - val_loss: 0.0809 - val_accuracy: 0.9790\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.0951 - accuracy: 0.9675 - val_loss: 0.0799 - val_accuracy: 0.9780\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.1164 - accuracy: 0.9645 - val_loss: 0.0791 - val_accuracy: 0.9780\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 13s 135ms/step - loss: 0.0870 - accuracy: 0.9690 - val_loss: 0.0771 - val_accuracy: 0.9790\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.1077 - accuracy: 0.9575 - val_loss: 0.0780 - val_accuracy: 0.9790\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.1034 - accuracy: 0.9620 - val_loss: 0.0782 - val_accuracy: 0.9790\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 13s 135ms/step - loss: 0.0953 - accuracy: 0.9665 - val_loss: 0.0814 - val_accuracy: 0.9760\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 13s 135ms/step - loss: 0.0849 - accuracy: 0.9685 - val_loss: 0.0842 - val_accuracy: 0.9760\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.0874 - accuracy: 0.9675 - val_loss: 0.0784 - val_accuracy: 0.9780\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 14s 141ms/step - loss: 0.0923 - accuracy: 0.9675 - val_loss: 0.0733 - val_accuracy: 0.9800\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.0897 - accuracy: 0.9700 - val_loss: 0.0801 - val_accuracy: 0.9770\n"
     ]
    }
   ],
   "source": [
    "model_base.trainable=True\n",
    "\n",
    "# 상위 layer 동결해제\n",
    "for layer in model_base.layers:\n",
    "    if layer.name in ['top_conv', 'block7b_project_conv']:\n",
    "        layer.trainable=True\n",
    "    else:\n",
    "        layer.trainable=False\n",
    "\n",
    "# 미세조정이므로 learning_rate를 더 작게 설정\n",
    "model.compile(optimizer=RMSprop(learning_rate=1e-5), loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_generator, steps_per_epoch=100, epochs=20,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env_tf2] *",
   "language": "python",
   "name": "conda-env-data_env_tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
