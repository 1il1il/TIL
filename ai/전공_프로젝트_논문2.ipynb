{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # 텐서플로가 첫 번째 GPU에 1GB 메모리만 할당하도록 제한\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*6)])\n",
    "  except RuntimeError as e:\n",
    "    # 프로그램 시작시에 가상 장치가 설정되어야만 합니다\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15986 images belonging to 7 classes.\n",
      "Found 3992 images belonging to 7 classes.\n",
      "Found 2074 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "base_dir = '../data/telemoji/pre'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1/255,    \n",
    "#        rotation_range=40,\n",
    "#        width_shift_range=0.1,\n",
    "#        height_shift_range=0.1,\n",
    "#        zoom_range=0.2,\n",
    "#        horizontal_flip=True,\n",
    "#        vertical_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "     rescale=1/255\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, \n",
    "    color_mode='grayscale',\n",
    "    target_size=(150,150),\n",
    "    batch_size=100,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    color_mode='grayscale',\n",
    "    target_size=(150,150),\n",
    "    batch_size=100,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    color_mode='grayscale',\n",
    "    target_size=(150,150),\n",
    "    batch_size=100,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### block 1 ###\n",
    "# filter 1\n",
    "W1 = tf.Variable(tf.random.normal([3, 3, 1, 64]))\n",
    "\n",
    "# convolution 1\n",
    "C1 = tf.nn.conv2d(train_generator, W1, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C1 = tf.nn.relu(C1)\n",
    "\n",
    "# filter 2\n",
    "W2 = tf.Variable(tf.random.normal([3, 3, 64, 64]))\n",
    "\n",
    "# convolution 2\n",
    "C2 = tf.nn.conv2d(C1, W2, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C2 = tf.nn.relu(C2)\n",
    "\n",
    "# max pooling 1\n",
    "P1 = tf.nn.max_pool(C2, ksize=[1, 2, 2, 1], \n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# > (?,76,76,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 26, 26, 2)\n"
     ]
    }
   ],
   "source": [
    "input_shape = (4, 28, 28, 3)\n",
    "\n",
    "x = tf.random.normal(input_shape)\n",
    "C1 = tf.keras.layers.Conv2D(\n",
    "filters=2, kernel_size=3, activation='relu', input_shape=input_shape[1:]\n",
    ")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### block 2 ###\n",
    "# filter 3\n",
    "W3 = tf.Variable(tf.random.normal([3, 3, 64, 128]))\n",
    "\n",
    "# convolution 3\n",
    "C3 = tf.nn.conv2d(P1, W3, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C3 = tf.nn.relu(C3)\n",
    "\n",
    "# filter 4\n",
    "W4 = tf.Variable(tf.random.normal([3, 3, 128, 128]))\n",
    "\n",
    "# convolution 4\n",
    "C4 = tf.nn.conv2d(C3, W4, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C4 = tf.nn.relu(C4)\n",
    "\n",
    "# filter 5\n",
    "W5 = tf.Variable(tf.random.normal([3, 3, 128, 128]))\n",
    "\n",
    "# convolution 5\n",
    "C5 = tf.nn.conv2d(C4, W5, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C5 = tf.nn.relu(C5)\n",
    "\n",
    "# pooling 2\n",
    "P2 = tf.nn.max_pool(C5, ksize=[1, 2, 2, 1], \n",
    "                    strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "# > (?,38,38,128)\n",
    "\n",
    "# FC 1\n",
    "FC1 = tf.reshape(P2, [-1, 38*38*128])\n",
    "\n",
    "W_FC1 = tf.Variable(tf.random.normal([38*38*128, 256]))\n",
    "b_FC1 = tf.Variable(tf.random.normal([256]))\n",
    "FC1 = tf.nn.relu(tf.matmul(FC1, W_FC1) + b_FC1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### block 3 ###\n",
    "# filter 6\n",
    "W6 = tf.Variable(tf.random.normal([3, 3, 128, 256]))\n",
    "\n",
    "# convolution 6\n",
    "C6 = tf.nn.conv2d(P2, W6, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C6 = tf.nn.relu(C6)\n",
    "\n",
    "# filter 7\n",
    "W7 = tf.Variable(tf.random.normal([3, 3, 256, 256]))\n",
    "\n",
    "# convolution 7\n",
    "C7 = tf.nn.conv2d(C6, W7, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C7 = tf.nn.relu(C7)\n",
    "\n",
    "# filter 8\n",
    "W8 = tf.Variable(tf.random.normal([3, 3, 256, 256]))\n",
    "\n",
    "# convolution 8\n",
    "C8 = tf.nn.conv2d(C7, W8, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C8 = tf.nn.relu(C8)\n",
    "\n",
    "# filter 9\n",
    "W9 = tf.Variable(tf.random.normal([3, 3, 256, 256]))\n",
    "\n",
    "# convolution 9\n",
    "C9 = tf.nn.conv2d(C8, W9, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C9 = tf.nn.relu(C9)\n",
    "\n",
    "# pooling 3\n",
    "P3 = tf.nn.max_pool(C9, ksize=[1, 2, 2, 1], \n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# > (?,20,20,256)\n",
    "\n",
    "# FC 2\n",
    "FC2 = tf.reshape(P3, [-1, 20*20*256])\n",
    "\n",
    "W_FC2 = tf.Variable(tf.random.normal([20*20*256, 256]))\n",
    "b_FC2 = tf.Variable(tf.random.normal([256]))\n",
    "FC2 = tf.nn.relu(tf.matmul(FC2, W_FC2) + b_FC2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### block 4 ###\n",
    "# filter 10\n",
    "W10 = tf.Variable(tf.random.normal([3, 3, 256, 256]))\n",
    "\n",
    "# convolution 10\n",
    "C10 = tf.nn.conv2d(P3, W10, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C10 = tf.nn.relu(C10)\n",
    "\n",
    "# filter 11\n",
    "W11 = tf.Variable(tf.random.normal([3, 3, 256, 256]))\n",
    "\n",
    "# convolution 11\n",
    "C11 = tf.nn.conv2d(C10, W11, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C11 = tf.nn.relu(C11)\n",
    "\n",
    "# filter 12\n",
    "W12 = tf.Variable(tf.random.normal([3, 3, 256, 256]))\n",
    "\n",
    "# convolution 12\n",
    "C12 = tf.nn.conv2d(C11, W12, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C12 = tf.nn.relu(C12)\n",
    "\n",
    "# filter 13\n",
    "W13 = tf.Variable(tf.random.normal([3, 3, 256, 256]))\n",
    "\n",
    "# convolution 13\n",
    "C13 = tf.nn.conv2d(C12, W13, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C13 = tf.nn.relu(C13)\n",
    "\n",
    "# pooling 4\n",
    "P4 = tf.nn.max_pool(C13, ksize=[1, 2, 2, 1], \n",
    "                    strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "# > (?,10,10,256)\n",
    "\n",
    "# FC 3\n",
    "FC3 = tf.reshape(P2, [-1, 10*10*256])\n",
    "\n",
    "W_FC3 = tf.Variable(tf.random.normal([10*10*256, 256]))\n",
    "b_FC3 = tf.Variable(tf.random.normal([256]))\n",
    "FC3 = tf.nn.relu(tf.matmul(FC3, W_FC3) + b_FC3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### block 5 ###\n",
    "# filter 14\n",
    "W14 = tf.Variable(tf.random.normal([3, 3, 256, 512]))\n",
    "\n",
    "# convolution 14\n",
    "C14 = tf.nn.conv2d(P4, W14, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C14 = tf.nn.relu(C14)\n",
    "\n",
    "# filter 15\n",
    "W15 = tf.Variable(tf.random.normal([3, 3, 512, 512]))\n",
    "\n",
    "# convolution 15\n",
    "C15 = tf.nn.conv2d(C14, W15, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C15 = tf.nn.relu(C15)\n",
    "\n",
    "# filter 16\n",
    "W16 = tf.Variable(tf.random.normal([3, 3, 512, 512]))\n",
    "\n",
    "# convolution 16\n",
    "C16 = tf.nn.conv2d(C15, W16, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C16 = tf.nn.relu(C16)\n",
    "\n",
    "# filter 17\n",
    "W17 = tf.Variable(tf.random.normal([3, 3, 512, 512]))\n",
    "\n",
    "# convolution 17\n",
    "C17 = tf.nn.conv2d(C16, W17, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C17 = tf.nn.relu(C17)\n",
    "\n",
    "# pooling 5\n",
    "P5 = tf.nn.max_pool(C17, ksize=[1, 2, 2, 1], \n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# > (?,6,6,512)\n",
    "\n",
    "\n",
    "# flatten\n",
    "# DNN으로 들어가야할 것은 한개의 이미지에 대한 데이터\n",
    "P5 = tf.reshape(P5, [-1, 6*6*512])\n",
    "\n",
    "# concat\n",
    "P5 = tf.concat([FC1, FC2, FC3, P5], 0)\n",
    "\n",
    "W18 = tf.Variable(tf.random.normal([(38*38*128)+(20*20*256)+(10*10*256)+(6*6*512), 7]))\n",
    "b18 = tf.Variable(tf.random.normal([7]))\n",
    "\n",
    "# hypothesis\n",
    "logit = tf.matmul(P5, W18) + b18\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, \n",
    "                                                                 labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env_tf2] *",
   "language": "python",
   "name": "conda-env-data_env_tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
