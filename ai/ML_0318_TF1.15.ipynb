{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "configured-performance",
   "metadata": {},
   "source": [
    "# MNIST TF 1.15 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "declared-glass",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "indirect-parker",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/digit-recognizer/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "coral-republic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 결측치, 이상치 없음\n",
    "# 이미지 확인\n",
    "img_data = df.drop('label', axis=1, inplace=False).values\n",
    "\n",
    "plt.imshow(img_data[0].reshape(-1,28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abroad-archives",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "x_data_train, x_data_test, t_data_train, t_data_test = \\\n",
    "train_test_split(df.drop('label', axis=1, inplace=False), df['label'],\n",
    "                 test_size=0.3, random_state=0)\n",
    "\n",
    "# Normalization\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_data_train)\n",
    "\n",
    "x_data_train_norm = scaler.transform(x_data_train)\n",
    "x_data_test_norm = scaler.transform(x_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow 구현\n",
    "\n",
    "# One-hot encording\n",
    "sess = tf.Session()\n",
    "t_data_train_onehot = sess.run(tf.one_hot(t_data_train, depth=10))\n",
    "t_data_test_onehot = sess.run(tf.one_hot(t_data_test, depth=10))\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "\n",
    "# W & b (hidden layer 1)\n",
    "W2 = tf.Variable(tf.random.normal([784, 64]))\n",
    "b2 = tf.Variable(tf.random.normal([64]))\n",
    "layer2 = tf.sigmoid(tf.matmul(X, W2) + b2)\n",
    "\n",
    "# hidden layer 2\n",
    "W3 = tf.Variable(tf.random.normal([64, 32]))\n",
    "b3 = tf.Variable(tf.random.normal([32]))\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "# hidden layer 3\n",
    "W4 = tf.Variable(tf.random.normal([32, 16]))\n",
    "b4 = tf.Variable(tf.random.normal([16]))\n",
    "layer4 = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
    "\n",
    "# output layer\n",
    "W5 = tf.Variable(tf.random.normal([16, 10]))\n",
    "b5 = tf.Variable(tf.random.normal([10]))\n",
    "\n",
    "logit = tf.matmul(layer4, W5) +b5\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "# loss fuction\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, \n",
    "                                                                 labels=T))\n",
    "\n",
    "# train \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
    "\n",
    "# initializer\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning\n",
    "for step in range(10000):\n",
    "    _, loss_val = sess.run([train, loss], feed_dict={X: x_data_train_norm, \n",
    "                                                     T: t_data_train_onehot})\n",
    "    \n",
    "    if step % 1000 == 0:\n",
    "        print('loss: {}'.format(loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "predict = tf.argmax(H, 1)\n",
    "correct = tf.equal(predict, tf.argmax(T, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "result = sess.run(accuracy, feed_dict={X: x_data_test_norm, \n",
    "                                       T: t_data_test_onehot})\n",
    "print('정확도 : {}'.format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-heather",
   "metadata": {},
   "source": [
    "## weight의 초기값을 지정하는 방법 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-increase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow 구현\n",
    "\n",
    "# One-hot encording\n",
    "sess = tf.Session()\n",
    "t_data_train_onehot = sess.run(tf.one_hot(t_data_train, depth=10))\n",
    "t_data_test_onehot = sess.run(tf.one_hot(t_data_test, depth=10))\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "\n",
    "# W & b (hidden layer 1)\n",
    "# Xavier Initialization \n",
    "# 입력의 개수와 출력의 개수를 이용해서 Weight의 초기값을 결정하는 방법\n",
    "# W = np.random.randn(num_of_input, num_of_output) / np.squrt(num_of_input)\n",
    "# tf.contrib.layers.xavier_initializer()\n",
    "# He's Initialization : Xavier Initialization의 확장 버전\n",
    "# W = np.random.randn(num_of_input, num_of_output) / np.squrt(num_of_input/2)\n",
    "# # tf.contrib.layers.variance_scaling_initializer \n",
    "\n",
    "W2 = tf.get_variable('W2', shape=[784, 64],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random.normal([64]))\n",
    "layer2 = tf.nn.relu(tf.matmul(X, W2) + b2)\n",
    "\n",
    "# hidden layer 2\n",
    "W3 = tf.get_variable('W3', shape=[64, 32], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random.normal([32]))\n",
    "layer3 = tf.nn.relu(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "# hidden layer 3\n",
    "W4 = tf.get_variable('W4', shape=[32, 16], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random.normal([16]))\n",
    "layer4 = tf.nn.relu(tf.matmul(layer3, W4) + b4)\n",
    "\n",
    "# output layer\n",
    "\n",
    "W5 = tf.get_variable('W5', shape=[16, 10], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random.normal([10]))\n",
    "\n",
    "logit = tf.matmul(layer4, W5) +b5\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "# loss fuction\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, \n",
    "                                                                 labels=T))\n",
    "\n",
    "# train \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
    "\n",
    "# initializer\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning\n",
    "for step in range(5000):\n",
    "    _, loss_val = sess.run([train, loss], feed_dict={X: x_data_train_norm, \n",
    "                                                     T: t_data_train_onehot})\n",
    "    \n",
    "    if step % 500 == 0:\n",
    "        print('loss: {}'.format(loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "predict = tf.argmax(H, 1)\n",
    "correct = tf.equal(predict, tf.argmax(T, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "result = sess.run(accuracy, feed_dict={X: x_data_test_norm, \n",
    "                                       T: t_data_test_onehot})\n",
    "print('정확도 : {}'.format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-denial",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "quantitative-observation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# One-hot encording\n",
    "sess = tf.Session()\n",
    "t_data_train_onehot = sess.run(tf.one_hot(t_data_train, depth=10))\n",
    "t_data_test_onehot = sess.run(tf.one_hot(t_data_test, depth=10))\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "\n",
    "# W & b (hidden layer 1)\n",
    "W2 = tf.get_variable('W2', shape=[784, 64],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random.normal([64]))\n",
    "_layer2 = tf.nn.relu(tf.matmul(X, W2) + b2)\n",
    "layer2 = tf.nn.dropout(_layer2, rate=0.3)\n",
    "\n",
    "# hidden layer 2\n",
    "W3 = tf.get_variable('W3', shape=[64, 32], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random.normal([32]))\n",
    "_layer3 = tf.nn.relu(tf.matmul(layer2, W3) + b3)\n",
    "layer3 = tf.nn.dropout(_layer3, rate=0.3)\n",
    "\n",
    "# hidden layer 3\n",
    "W4 = tf.get_variable('W4', shape=[32, 16], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random.normal([16]))\n",
    "_layer4 = tf.nn.relu(tf.matmul(layer3, W4) + b4)\n",
    "layer4 = tf.nn.dropout(_layer4, rate=0.3)\n",
    "\n",
    "# output layer\n",
    "W5 = tf.get_variable('W5', shape=[16, 10], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random.normal([10]))\n",
    "\n",
    "logit = tf.matmul(layer4, W5) +b5\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "# loss fuction\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, \n",
    "                                                                 labels=T))\n",
    "\n",
    "# train \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
    "\n",
    "# initializer\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "shared-scene",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.784061849117279\n",
      "loss: 0.673759937286377\n",
      "loss: 0.613986611366272\n",
      "loss: 0.5663807392120361\n",
      "loss: 0.5256711840629578\n",
      "loss: 0.48989272117614746\n",
      "loss: 0.46064406633377075\n",
      "loss: 0.43761175870895386\n",
      "loss: 0.41839364171028137\n",
      "loss: 0.40192481875419617\n"
     ]
    }
   ],
   "source": [
    "# Learning\n",
    "for step in range(5000):\n",
    "    _, loss_val = sess.run([train, loss], feed_dict={X: x_data_train_norm, \n",
    "                                                     T: t_data_train_onehot})\n",
    "    \n",
    "    if step % 500 == 0:\n",
    "        print('loss: {}'.format(loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "internal-fighter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.8653967976570129\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "predict = tf.argmax(H, 1)\n",
    "correct = tf.equal(predict, tf.argmax(T, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "result = sess.run(accuracy, feed_dict={X: x_data_test_norm, \n",
    "                                       T: t_data_test_onehot})\n",
    "print('정확도 : {}'.format(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env]",
   "language": "python",
   "name": "conda-env-data_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
