{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "df = pd.read_csv('../data/archive/fashion-mnist_train.csv')\n",
    "display(df.shape)\n",
    "\n",
    "# training test split\n",
    "x_train, x_test, t_train, t_test =\\\n",
    "train_test_split(df.drop(columns=['label']), pd.get_dummies(df['label']), \n",
    "                 test_size=0.2, random_state=0)\n",
    "\n",
    "# Normaliztion\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_norm = scaler.transform(x_train)\n",
    "x_test_norm = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutinomial Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_dim = x_train.shape[1]\n",
    "t_dim = t_train.shape[1]\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None, x_dim], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None, t_dim], dtype=tf.float32)\n",
    "\n",
    "# W & b\n",
    "W = tf.get_variable('W', shape=[x_dim, t_dim],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.Variable(tf.random.normal([t_dim]))\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(X, W) + b\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
    "\n",
    "# session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# learning\n",
    "for step in range(1000):\n",
    "    _,loss_val = sess.run([train,loss], feed_dict={X:x_train_norm, T:t_train})\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(f'loss: {loss_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# accuracy\n",
    "predict = np.argmax(sess.run(H, feed_dict = {X: x_test_norm}), axis=1)\n",
    "acc_list = np.equal(predict, np.argmax(t_test.values, axis=1))\n",
    "print('accuracy : {}'.format(acc_list.sum()/len(acc_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Nueral Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "df = pd.read_csv('../data/archive/fashion-mnist_train.csv')\n",
    "display(df.shape)\n",
    "\n",
    "# training test split\n",
    "x_train, x_test, t_train, t_test =\\\n",
    "train_test_split(df.drop(columns=['label']), pd.get_dummies(df['label']), \n",
    "                 test_size=0.2, random_state=0)\n",
    "\n",
    "# Normaliztion\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_norm = scaler.transform(x_train)\n",
    "x_test_norm = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_dim = x_train.shape[1]\n",
    "t_dim = t_train.shape[1]\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None, x_dim], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None, t_dim], dtype=tf.float32)\n",
    "\n",
    "# W & b\n",
    "W2 = tf.get_variable('W2', shape=[x_dim, 64],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random.normal([64]))\n",
    "layer2 = tf.nn.relu(tf.matmul(X, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable('W3', shape=[64, 32], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random.normal([32]))\n",
    "layer3 = tf.nn.relu(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "W4 = tf.get_variable('W4', shape=[32, 16], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random.normal([16]))\n",
    "layer4 = tf.nn.relu(tf.matmul(layer3, W4) + b4)\n",
    "\n",
    "W5 = tf.get_variable('W5', shape=[16, t_dim], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random.normal([t_dim]))\n",
    "\n",
    "# hypothesis\n",
    "logit = tf.matmul(layer4, W5) +b5\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, \n",
    "                                                                 labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
    "\n",
    "# session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# learning\n",
    "for step in range(1000):\n",
    "    _,loss_val = sess.run([train,loss], feed_dict={X:x_train_norm, T:t_train})\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(f'loss: {loss_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# accuracy\n",
    "predict = np.argmax(sess.run(H, feed_dict = {X: x_test_norm}), axis=1)\n",
    "acc_list = np.equal(predict, np.argmax(t_test.values, axis=1))\n",
    "print('accuracy : {}'.format(acc_list.sum()/len(acc_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "df = pd.read_csv('../data/archive/fashion-mnist_train.csv')\n",
    "\n",
    "# training test split\n",
    "x_train, x_test, t_train, t_test =\\\n",
    "train_test_split(df.drop(columns=['label']).values, \n",
    "                 pd.get_dummies(df['label']).values, \n",
    "                 test_size=0.2, random_state=0)\n",
    "\n",
    "# Normaliztion\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_norm = scaler.transform(x_train)\n",
    "x_test_norm = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태 변환\n",
    "x_train_norm = x_train_norm.reshape((x_train_norm.shape[0], ) + (28, 28, 1))\n",
    "x_test_norm = x_test_norm.reshape((x_test_norm.shape[0], ) + (28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# placeholder\n",
    "X = tf.placeholder(shape=[None, 28, 28 ,1], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "\n",
    "# filter\n",
    "weight = tf.get_variable('filter', shape=[3, 3, 1, 1],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "# convolution\n",
    "conv2d = tf.nn.conv2d(X, weight, \n",
    "                      strides=[1,1,1,1], padding='VALID')\n",
    "\n",
    "# relu\n",
    "relu = tf.nn.relu(conv2d)\n",
    "\n",
    "# pooling\n",
    "pooling = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], \n",
    "                         strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "# filter 2\n",
    "weight2 = tf.get_variable('filter2', shape=[3, 3, 1, 1],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "# convolution 2\n",
    "conv2d2 = tf.nn.conv2d(pooling, weight, \n",
    "                      strides=[1,1,1,1], padding='VALID')\n",
    "\n",
    "# relu 2\n",
    "relu2 = tf.nn.relu(conv2d2)\n",
    "\n",
    "# pooling 2\n",
    "pooling2 = tf.nn.max_pool(relu2, ksize=[1, 2, 2, 1], \n",
    "                         strides=[1, 2, 2, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten\n",
    "flatten = tf.layers.Flatten()(pooling2)\n",
    "\n",
    "# W & b\n",
    "W = tf.get_variable('W', shape=[25, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.Variable(tf.random.normal([10]))\n",
    "\n",
    "# hypothesis\n",
    "logit = tf.matmul(flatten, W) +b\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, \n",
    "                                                                 labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
    "\n",
    "# session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.7932090759277344\n",
      "loss: 0.7379981875419617\n",
      "loss: 0.7116959691047668\n",
      "loss: 0.7014251351356506\n",
      "loss: 0.6956386566162109\n",
      "loss: 0.6907168030738831\n",
      "loss: 0.6872500777244568\n",
      "loss: 0.6840869784355164\n",
      "loss: 0.6808200478553772\n",
      "loss: 0.6778950095176697\n"
     ]
    }
   ],
   "source": [
    "# learning\n",
    "for step in range(10000):\n",
    "    _,loss_val = sess.run([train,loss], feed_dict={X:x_train_norm, T:t_train})\n",
    "    \n",
    "    if step % 1000 == 0:\n",
    "        print(f'loss: {loss_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.7579166666666667\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "predict = np.argmax(sess.run(H, feed_dict = {X: x_test_norm}), axis=1)\n",
    "acc_list = np.equal(predict, np.argmax(t_test, axis=1))\n",
    "print('accuracy : {}'.format(acc_list.sum()/len(acc_list)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env] *",
   "language": "python",
   "name": "conda-env-data_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
