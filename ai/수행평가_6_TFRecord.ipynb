{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "applied-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.applications import EfficientNetB3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "biblical-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # 텐서플로가 첫 번째 GPU에 1GB 메모리만 할당하도록 제한\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*6)])\n",
    "  except RuntimeError as e:\n",
    "    # 프로그램 시작시에 가상 장치가 설정되어야만 합니다\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "serious-anime",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, validation TFRecord 파일 경로\n",
    "train_tfrecord_path = '../data/cat_dog/cat_dog_train.tfrecords'\n",
    "valid_tfrecord_path = '../data/cat_dog/cat_dog_valid.tfrecords'\n",
    "\n",
    "BUFFER_SIZE = 256     # 데이터 shuffle을 위한 buffer size\n",
    "BATCH_SIZE = 100       # 배치 사이즈. 한번에 가져오는 이미지 데이터 개수 \n",
    "NUM_CLASS = 2         # class의 개수. binary인 경우는 필요없으며 categorical인 경우 설정\n",
    "IMAGE_SIZE = 150       \n",
    "\n",
    "\n",
    "# TFRecord를 읽어서 데이터를 복원하기 위한 자료구조.\n",
    "image_feature_description = {\n",
    "    'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "    'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'id': tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "# 읽어들인 TFRecord를 다음의 형태(dict)로 변환하는 함수\n",
    "def _parse_image_function(example_proto):\n",
    "    return tf.io.parse_single_example(example_proto, \n",
    "                                      image_feature_description)\n",
    "\n",
    "# 위에서 얻은 ParallelMapDataset를 다음의 형태(shape)로 변환하는 함수\n",
    "def map_func(target_record):\n",
    "    img = target_record['image_raw']\n",
    "    label = target_record['label']\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.dtypes.cast(img, tf.float32)\n",
    "    return img, label\n",
    "\n",
    "\n",
    "# 전처리(resize & augmentation) 함수\n",
    "def image_resize_func(image, label):\n",
    "    # result_image = image / 255\n",
    "    result_image = tf.image.resize(image, (IMAGE_SIZE,IMAGE_SIZE))   \n",
    "    return result_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "swiss-relay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.TFRecordDataset(train_tfrecord_path, \n",
    "                                  compression_type='GZIP')\n",
    "# dataset 길이 확인\n",
    "dataset_length = [i for i,_ in enumerate(train_dataset)][-1] + 1\n",
    "print(dataset_length)\n",
    "\n",
    "train_dataset = train_dataset.map(_parse_image_function, \n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.map(map_func, \n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# train_dataset = train_dataset.cache()\n",
    "# dataset shuffle 처리\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# 전처리(resize)\n",
    "train_dataset = train_dataset.map(image_resize_func, \n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# BatchDataset으로 변환\n",
    "# <BatchDataset shapes: ((None, None, None, 3), (None,)), types: (tf.float32, tf.int64)>\n",
    "# BatchDataset으로 변환하기 전에 image의 resize(전처리)가 일어나야 한다. 그렇지 않으면 \n",
    "# shape이 달라 batch처리가 되지 않는다는 오류 발생.\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).repeat()\n",
    "\n",
    "# # dataset 길이 확인\n",
    "# dataset_length = [i for i,_ in enumerate(train_dataset)][-1] + 1\n",
    "# print(dataset_length)\n",
    "\n",
    "# prefetch처리\n",
    "# prefetch는 전처리와 학습과정의 모델 실행을 오버랩.\n",
    "# 모델이 s스텝 학습을 실행하는 동안 입력 파이프라인은 s+1스텝의 데이터를 읽어서 수행속도를 높임.\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "average-trick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = tf.data.TFRecordDataset(valid_tfrecord_path, \n",
    "                                  compression_type='GZIP')\n",
    "\n",
    "# dataset 길이 확인\n",
    "dataset_length = [i for i,_ in enumerate(valid_dataset)][-1] + 1\n",
    "print(dataset_length)\n",
    "\n",
    "valid_dataset = valid_dataset.map(_parse_image_function, \n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.map(map_func, \n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# valid_dataset = valid_dataset.cache()\n",
    "# dataset shuffle 처리\n",
    "valid_dataset = valid_dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# 전처리(resize)\n",
    "valid_dataset = valid_dataset.map(image_resize_func, \n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# BatchDataset으로 변환\n",
    "# <BatchDataset shapes: ((None, None, None, 3), (None,)), types: (tf.float32, tf.int64)>\n",
    "# BatchDataset으로 변환하기 전에 image의 resize(전처리)가 일어나야 한다. 그렇지 않으면 \n",
    "# shape이 달라 batch처리가 되지 않는다는 오류 발생.\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE).repeat()\n",
    "\n",
    "# # dataset 길이 확인\n",
    "# dataset_length = [i for i,_ in enumerate(valid_dataset)][-1] + 1\n",
    "# print(dataset_length)\n",
    "\n",
    "# prefetch처리\n",
    "# prefetch는 전처리와 학습과정의 모델 실행을 오버랩.\n",
    "# 모델이 s스텝 학습을 실행하는 동안 입력 파이프라인은 s+1스텝의 데이터를 읽어서 수행속도를 높임.\n",
    "valid_dataset = valid_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# # dataset 길이 확인\n",
    "# dataset_length = [i for i,_ in enumerate(valid_dataset)][-1] + 1\n",
    "# print(dataset_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_length = [i for i,_ in enumerate(train_dataset)][-1] + 1\n",
    "print(dataset_length)\n",
    "\n",
    "dataset_length = [i for i,_ in enumerate(valid_dataset)][-1] + 1\n",
    "print(dataset_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "artistic-going",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnetb3 (Functional)  (None, 5, 5, 1536)        10783535  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 38400)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               9830656   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 20,614,448\n",
      "Trainable params: 9,830,913\n",
      "Non-trainable params: 10,783,535\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "100/100 [==============================] - 37s 366ms/step - loss: 0.1094 - accuracy: 0.9607 - val_loss: 0.0520 - val_accuracy: 0.9806\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 34s 339ms/step - loss: 0.0687 - accuracy: 0.9748 - val_loss: 0.0463 - val_accuracy: 0.9824\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 34s 342ms/step - loss: 0.0535 - accuracy: 0.9809 - val_loss: 0.0448 - val_accuracy: 0.9830\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 34s 343ms/step - loss: 0.0502 - accuracy: 0.9818 - val_loss: 0.0435 - val_accuracy: 0.9834\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.0432 - accuracy: 0.9854 - val_loss: 0.0439 - val_accuracy: 0.9834\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.0366 - accuracy: 0.9873 - val_loss: 0.0436 - val_accuracy: 0.9840\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 34s 344ms/step - loss: 0.0373 - accuracy: 0.9868 - val_loss: 0.0435 - val_accuracy: 0.9828\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 34s 344ms/step - loss: 0.0307 - accuracy: 0.9895 - val_loss: 0.0425 - val_accuracy: 0.9842\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 0.0271 - accuracy: 0.9917 - val_loss: 0.0433 - val_accuracy: 0.9838\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 34s 344ms/step - loss: 0.0250 - accuracy: 0.9923 - val_loss: 0.0438 - val_accuracy: 0.9836\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.0242 - accuracy: 0.9926 - val_loss: 0.0442 - val_accuracy: 0.9846\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.0221 - accuracy: 0.9923 - val_loss: 0.0445 - val_accuracy: 0.9852\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 0.0236 - accuracy: 0.9933 - val_loss: 0.0477 - val_accuracy: 0.9834\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 38s 376ms/step - loss: 0.0207 - accuracy: 0.9936 - val_loss: 0.0460 - val_accuracy: 0.9836\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0189 - accuracy: 0.9939 - val_loss: 0.0454 - val_accuracy: 0.9836\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 38s 377ms/step - loss: 0.0162 - accuracy: 0.9951 - val_loss: 0.0482 - val_accuracy: 0.9842\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 0.0157 - accuracy: 0.9953 - val_loss: 0.0490 - val_accuracy: 0.9844\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 35s 345ms/step - loss: 0.0129 - accuracy: 0.9957 - val_loss: 0.0479 - val_accuracy: 0.9848\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 34s 344ms/step - loss: 0.0158 - accuracy: 0.9949 - val_loss: 0.0504 - val_accuracy: 0.9830\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 0.0133 - accuracy: 0.9965 - val_loss: 0.0510 - val_accuracy: 0.9838\n"
     ]
    }
   ],
   "source": [
    "# pretrained network\n",
    "model_base = EfficientNetB3(include_top=False, weights='imagenet', input_shape=(150,150,3))\n",
    "# model_base의 weight학습을 동결\n",
    "model_base.trainable=False\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# pretrained network를 우리의 모델 앞에 추가\n",
    "model.add(model_base)\n",
    "\n",
    "model.add(Flatten(input_shape=(4*4*512,)))\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(learning_rate=2e-5), loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_dataset, steps_per_epoch=100, epochs=20,\n",
    "                    validation_data=valid_dataset,\n",
    "                    validation_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "growing-farming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 37s 374ms/step - loss: 0.0139 - accuracy: 0.9952 - val_loss: 0.0493 - val_accuracy: 0.9842\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 36s 364ms/step - loss: 0.0093 - accuracy: 0.9974 - val_loss: 0.0490 - val_accuracy: 0.9840\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 37s 367ms/step - loss: 0.0124 - accuracy: 0.9958 - val_loss: 0.0497 - val_accuracy: 0.9848\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 36s 362ms/step - loss: 0.0071 - accuracy: 0.9980 - val_loss: 0.0513 - val_accuracy: 0.9844\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 39s 389ms/step - loss: 0.0102 - accuracy: 0.9968 - val_loss: 0.0526 - val_accuracy: 0.9842\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 37s 370ms/step - loss: 0.0081 - accuracy: 0.9973 - val_loss: 0.0514 - val_accuracy: 0.9856\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 36s 360ms/step - loss: 0.0096 - accuracy: 0.9975 - val_loss: 0.0552 - val_accuracy: 0.9844\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 36s 358ms/step - loss: 0.0076 - accuracy: 0.9973 - val_loss: 0.0523 - val_accuracy: 0.9860\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.0101 - accuracy: 0.9970 - val_loss: 0.0521 - val_accuracy: 0.9852\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 36s 358ms/step - loss: 0.0087 - accuracy: 0.9974 - val_loss: 0.0522 - val_accuracy: 0.9852\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 36s 360ms/step - loss: 0.0087 - accuracy: 0.9979 - val_loss: 0.0513 - val_accuracy: 0.9848\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.0072 - accuracy: 0.9979 - val_loss: 0.0510 - val_accuracy: 0.9860\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.0090 - accuracy: 0.9974 - val_loss: 0.0528 - val_accuracy: 0.9838\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 35s 354ms/step - loss: 0.0060 - accuracy: 0.9983 - val_loss: 0.0534 - val_accuracy: 0.9858\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 35s 354ms/step - loss: 0.0100 - accuracy: 0.9969 - val_loss: 0.0540 - val_accuracy: 0.9842\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 35s 353ms/step - loss: 0.0078 - accuracy: 0.9968 - val_loss: 0.0513 - val_accuracy: 0.9854\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 35s 354ms/step - loss: 0.0083 - accuracy: 0.9972 - val_loss: 0.0507 - val_accuracy: 0.9854\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.0056 - accuracy: 0.9984 - val_loss: 0.0516 - val_accuracy: 0.9850\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 35s 351ms/step - loss: 0.0086 - accuracy: 0.9978 - val_loss: 0.0532 - val_accuracy: 0.9844\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.0066 - accuracy: 0.9986 - val_loss: 0.0530 - val_accuracy: 0.9856\n"
     ]
    }
   ],
   "source": [
    "model_base.trainable=True\n",
    "\n",
    "# 상위 layer 동결해제\n",
    "for layer in model_base.layers:\n",
    "    if layer.name in ['top_conv', 'block7b_project_conv']:\n",
    "        layer.trainable=True\n",
    "    else:\n",
    "        layer.trainable=False\n",
    "\n",
    "# 미세조정이므로 learning_rate를 더 작게 설정\n",
    "model.compile(optimizer=RMSprop(learning_rate=1e-5), loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_dataset, steps_per_epoch=100, epochs=20,\n",
    "                    validation_data=valid_dataset,\n",
    "                    validation_steps=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env_tf2] *",
   "language": "python",
   "name": "conda-env-data_env_tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
