{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "applied-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "biblical-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # 텐서플로가 첫 번째 GPU에 1GB 메모리만 할당하도록 제한\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*6)])\n",
    "  except RuntimeError as e:\n",
    "    # 프로그램 시작시에 가상 장치가 설정되어야만 합니다\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "serious-anime",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, validation TFRecord 파일 경로\n",
    "train_tfrecord_path = '../data/cat_dog/cat_dog_train.tfrecords'\n",
    "valid_tfrecord_path = '../data/cat_dog/cat_dog_valid.tfrecords'\n",
    "\n",
    "BUFFER_SIZE = 256     # 데이터 shuffle을 위한 buffer size\n",
    "BATCH_SIZE = 100       # 배치 사이즈. 한번에 가져오는 이미지 데이터 개수 \n",
    "NUM_CLASS = 2         # class의 개수. binary인 경우는 필요없으며 categorical인 경우 설정\n",
    "IMAGE_SIZE = 150       \n",
    "\n",
    "\n",
    "# TFRecord를 읽어서 데이터를 복원하기 위한 자료구조.\n",
    "image_feature_description = {\n",
    "    'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "    'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'id': tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "# 읽어들인 TFRecord를 다음의 형태(dict)로 변환하는 함수\n",
    "def _parse_image_function(example_proto):\n",
    "    return tf.io.parse_single_example(example_proto, \n",
    "                                      image_feature_description)\n",
    "\n",
    "# 위에서 얻은 ParallelMapDataset를 다음의 형태(shape)로 변환하는 함수\n",
    "def map_func(target_record):\n",
    "    img = target_record['image_raw']\n",
    "    label = target_record['label']\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.dtypes.cast(img, tf.float32)\n",
    "    return img, label\n",
    "\n",
    "\n",
    "# 전처리(resize & augmentation) 함수\n",
    "def image_resize_func(image, label):\n",
    "    # result_image = image / 255\n",
    "    result_image = tf.image.resize(image, (IMAGE_SIZE,IMAGE_SIZE))   \n",
    "    return result_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "swiss-relay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.TFRecordDataset(train_tfrecord_path, \n",
    "                                  compression_type='GZIP')\n",
    "# dataset 길이 확인\n",
    "dataset_length = [i for i,_ in enumerate(train_dataset)][-1] + 1\n",
    "print(dataset_length)\n",
    "\n",
    "train_dataset = train_dataset.map(_parse_image_function, \n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.map(map_func, \n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# train_dataset = train_dataset.cache()\n",
    "# dataset shuffle 처리\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# 전처리(resize)\n",
    "train_dataset = train_dataset.map(image_resize_func, \n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# BatchDataset으로 변환\n",
    "# <BatchDataset shapes: ((None, None, None, 3), (None,)), types: (tf.float32, tf.int64)>\n",
    "# BatchDataset으로 변환하기 전에 image의 resize(전처리)가 일어나야 한다. 그렇지 않으면 \n",
    "# shape이 달라 batch처리가 되지 않는다는 오류 발생.\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).repeat()\n",
    "\n",
    "# # dataset 길이 확인\n",
    "# dataset_length = [i for i,_ in enumerate(train_dataset)][-1] + 1\n",
    "# print(dataset_length)\n",
    "\n",
    "# prefetch처리\n",
    "# prefetch는 전처리와 학습과정의 모델 실행을 오버랩.\n",
    "# 모델이 s스텝 학습을 실행하는 동안 입력 파이프라인은 s+1스텝의 데이터를 읽어서 수행속도를 높임.\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "average-trick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = tf.data.TFRecordDataset(valid_tfrecord_path, \n",
    "                                  compression_type='GZIP')\n",
    "\n",
    "# dataset 길이 확인\n",
    "dataset_length = [i for i,_ in enumerate(valid_dataset)][-1] + 1\n",
    "print(dataset_length)\n",
    "\n",
    "valid_dataset = valid_dataset.map(_parse_image_function, \n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.map(map_func, \n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# valid_dataset = valid_dataset.cache()\n",
    "# dataset shuffle 처리\n",
    "valid_dataset = valid_dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# 전처리(resize)\n",
    "valid_dataset = valid_dataset.map(image_resize_func, \n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# BatchDataset으로 변환\n",
    "# <BatchDataset shapes: ((None, None, None, 3), (None,)), types: (tf.float32, tf.int64)>\n",
    "# BatchDataset으로 변환하기 전에 image의 resize(전처리)가 일어나야 한다. 그렇지 않으면 \n",
    "# shape이 달라 batch처리가 되지 않는다는 오류 발생.\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE).repeat()\n",
    "\n",
    "# # dataset 길이 확인\n",
    "# dataset_length = [i for i,_ in enumerate(valid_dataset)][-1] + 1\n",
    "# print(dataset_length)\n",
    "\n",
    "# prefetch처리\n",
    "# prefetch는 전처리와 학습과정의 모델 실행을 오버랩.\n",
    "# 모델이 s스텝 학습을 실행하는 동안 입력 파이프라인은 s+1스텝의 데이터를 읽어서 수행속도를 높임.\n",
    "valid_dataset = valid_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# # dataset 길이 확인\n",
    "# dataset_length = [i for i,_ in enumerate(valid_dataset)][-1] + 1\n",
    "# print(dataset_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_length = [i for i,_ in enumerate(train_dataset)][-1] + 1\n",
    "print(dataset_length)\n",
    "\n",
    "dataset_length = [i for i,_ in enumerate(valid_dataset)][-1] + 1\n",
    "print(dataset_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "artistic-going",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), \n",
    "                 activation='relu', input_shape=(150,150,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), \n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), \n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), \n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units=512, activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-3), \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "growing-farming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/200 [..............................] - ETA: 10s - loss: 150.9828 - accuracy: 0.4700WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0210s vs `on_train_batch_end` time: 0.0460s). Check your callbacks.\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 2.5372 - accuracy: 0.5275 - val_loss: 0.6824 - val_accuracy: 0.5678\n",
      "Epoch 2/30\n",
      "200/200 [==============================] - 16s 80ms/step - loss: 0.6793 - accuracy: 0.5588 - val_loss: 0.6779 - val_accuracy: 0.5710\n",
      "Epoch 3/30\n",
      "200/200 [==============================] - 16s 82ms/step - loss: 0.6818 - accuracy: 0.5731 - val_loss: 0.6686 - val_accuracy: 0.5964\n",
      "Epoch 4/30\n",
      "200/200 [==============================] - 17s 83ms/step - loss: 0.6657 - accuracy: 0.5876 - val_loss: 0.6442 - val_accuracy: 0.6200\n",
      "Epoch 5/30\n",
      "200/200 [==============================] - 17s 83ms/step - loss: 0.6347 - accuracy: 0.6438 - val_loss: 0.6438 - val_accuracy: 0.6474\n",
      "Epoch 6/30\n",
      "200/200 [==============================] - 16s 78ms/step - loss: 0.6109 - accuracy: 0.6673 - val_loss: 0.6118 - val_accuracy: 0.6646\n",
      "Epoch 7/30\n",
      "200/200 [==============================] - 17s 85ms/step - loss: 0.5698 - accuracy: 0.7035 - val_loss: 0.5681 - val_accuracy: 0.7088\n",
      "Epoch 8/30\n",
      "200/200 [==============================] - 17s 86ms/step - loss: 0.5204 - accuracy: 0.7431 - val_loss: 0.5117 - val_accuracy: 0.7498\n",
      "Epoch 9/30\n",
      "200/200 [==============================] - 17s 84ms/step - loss: 0.4615 - accuracy: 0.7814 - val_loss: 0.4763 - val_accuracy: 0.7780\n",
      "Epoch 10/30\n",
      "200/200 [==============================] - 17s 85ms/step - loss: 0.4198 - accuracy: 0.8086 - val_loss: 0.4389 - val_accuracy: 0.8066\n",
      "Epoch 11/30\n",
      "200/200 [==============================] - 17s 85ms/step - loss: 0.3710 - accuracy: 0.8371 - val_loss: 0.4256 - val_accuracy: 0.8162\n",
      "Epoch 12/30\n",
      "200/200 [==============================] - 17s 84ms/step - loss: 0.3296 - accuracy: 0.8571 - val_loss: 0.4285 - val_accuracy: 0.8226\n",
      "Epoch 13/30\n",
      "200/200 [==============================] - 16s 81ms/step - loss: 0.2992 - accuracy: 0.8737 - val_loss: 0.4188 - val_accuracy: 0.8274\n",
      "Epoch 14/30\n",
      "200/200 [==============================] - 17s 86ms/step - loss: 0.2600 - accuracy: 0.8914 - val_loss: 0.4328 - val_accuracy: 0.8216\n",
      "Epoch 15/30\n",
      "200/200 [==============================] - 17s 85ms/step - loss: 0.2326 - accuracy: 0.9039 - val_loss: 0.4446 - val_accuracy: 0.8332\n",
      "Epoch 16/30\n",
      "200/200 [==============================] - 17s 85ms/step - loss: 0.2132 - accuracy: 0.9115 - val_loss: 0.4393 - val_accuracy: 0.8380\n",
      "Epoch 17/30\n",
      "200/200 [==============================] - 17s 86ms/step - loss: 0.1801 - accuracy: 0.9255 - val_loss: 0.4607 - val_accuracy: 0.8358\n",
      "Epoch 18/30\n",
      "200/200 [==============================] - 17s 85ms/step - loss: 0.1749 - accuracy: 0.9313 - val_loss: 0.5033 - val_accuracy: 0.8268\n",
      "Epoch 19/30\n",
      "200/200 [==============================] - 17s 86ms/step - loss: 0.1658 - accuracy: 0.9342 - val_loss: 0.4809 - val_accuracy: 0.8442\n",
      "Epoch 20/30\n",
      "200/200 [==============================] - 16s 80ms/step - loss: 0.1402 - accuracy: 0.9456 - val_loss: 0.5256 - val_accuracy: 0.8400\n",
      "Epoch 21/30\n",
      "200/200 [==============================] - 17s 86ms/step - loss: 0.1280 - accuracy: 0.9512 - val_loss: 0.5325 - val_accuracy: 0.8464\n",
      "Epoch 22/30\n",
      "200/200 [==============================] - 17s 86ms/step - loss: 0.1110 - accuracy: 0.9582 - val_loss: 0.5953 - val_accuracy: 0.8462\n",
      "Epoch 23/30\n",
      "200/200 [==============================] - 17s 87ms/step - loss: 0.1032 - accuracy: 0.9616 - val_loss: 0.5743 - val_accuracy: 0.8556\n",
      "Epoch 24/30\n",
      "200/200 [==============================] - 17s 85ms/step - loss: 0.1146 - accuracy: 0.9575 - val_loss: 0.6057 - val_accuracy: 0.8480\n",
      "Epoch 25/30\n",
      "200/200 [==============================] - 17s 87ms/step - loss: 0.1061 - accuracy: 0.9611 - val_loss: 0.5042 - val_accuracy: 0.8506\n",
      "Epoch 26/30\n",
      "200/200 [==============================] - 17s 84ms/step - loss: 0.1018 - accuracy: 0.9624 - val_loss: 0.6139 - val_accuracy: 0.8448\n",
      "Epoch 27/30\n",
      "200/200 [==============================] - 16s 79ms/step - loss: 0.0820 - accuracy: 0.9693 - val_loss: 0.6227 - val_accuracy: 0.8468\n",
      "Epoch 28/30\n",
      "200/200 [==============================] - 17s 85ms/step - loss: 0.0890 - accuracy: 0.9674 - val_loss: 0.6378 - val_accuracy: 0.8420\n",
      "Epoch 29/30\n",
      "200/200 [==============================] - 17s 87ms/step - loss: 0.0857 - accuracy: 0.9699 - val_loss: 0.5507 - val_accuracy: 0.8606\n",
      "Epoch 30/30\n",
      "200/200 [==============================] - 17s 83ms/step - loss: 0.0761 - accuracy: 0.9718 - val_loss: 0.5916 - val_accuracy: 0.8590\n"
     ]
    }
   ],
   "source": [
    "# steps_per_epoch : 몇번 뽑아야 1 epoch이 되는가\n",
    "history = model.fit(train_dataset, steps_per_epoch=200, epochs=30,\n",
    "                    validation_data=valid_dataset,\n",
    "                    validation_steps=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env_tf2] *",
   "language": "python",
   "name": "conda-env-data_env_tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
