{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-morgan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-denmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = VGG16(weights='imagenet',\n",
    "                   include_top=False,\n",
    "                   input_shape=(150,150,3))\n",
    "model_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-environment",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../data/cat_dog_full'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1/255)\n",
    "batch_size = 20\n",
    "\n",
    "def extract_feature(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count,4,4,512))\n",
    "    labels = np.zeros(shape=(sample_count,))\n",
    "    \n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=(150,150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for x_data_batch, t_data_batch in generator:\n",
    "        feature_batch = model_base.predict(x_data_batch)\n",
    "        features[i*batch_size:(i+1)*batch_size] = feature_batch\n",
    "        labels[i*batch_size:(i+1)*batch_size] = t_data_batch\n",
    "        i += 1\n",
    "        \n",
    "        if i*batch_size >= sample_count:\n",
    "            break\n",
    "            \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = extract_feature(train_dir, 14000)\n",
    "validation_features, validation_labels = extract_feature(validation_dir, 6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-turtle",
   "metadata": {},
   "source": [
    "- Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.reshape(train_features, (14000, 4*4*512))\n",
    "validation_features = np.reshape(validation_features, (6000, 4*4*512))\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, activation='relu',\n",
    "                input_shape=(4*4*512,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=RMSprop(learning_rate=2e-5), loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-chest",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_features, train_labels, epochs=30,\n",
    "                    validation_data=(validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc =  history.history['val_accuracy']\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss =  history.history['val_loss']\n",
    "\n",
    "plt.plot(train_loss, color='b', label='training loss')\n",
    "plt.plot(val_loss, color='r', label='validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-liquid",
   "metadata": {},
   "source": [
    "- 조금 더 나은 결과를 얻으려면 데이터가 많아져야 할것 같음\n",
    "- 증식을 포함하여 다시 작성\n",
    "- pretrained network와 우리 classifier를 합쳐서 모델을 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-ridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-pickup",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../data/cat_dog_full'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1/255,    \n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, \n",
    "    target_size=(150,150),\n",
    "    batch_size=20,\n",
    "    classes=['cats','dogs'],\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir, \n",
    "    target_size=(150,150),\n",
    "    batch_size=20,\n",
    "    classes=['cats','dogs'],\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# pretrained network\n",
    "model_base = VGG16(include_top=False, weights='imagenet', input_shape=(150,150,3))\n",
    "# model_base의 weight학습을 동결\n",
    "model_base.trainable=False\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# pretrained network를 우리의 모델 앞에 추가\n",
    "model.add(model_base)\n",
    "\n",
    "model.add(Flatten(input_shape=(4*4*512,)))\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(learning_rate=2e-5), loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-bleeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator, steps_per_epoch=700, epochs=30,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-wayne",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "periodic-fishing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 16,812,353\n",
      "Trainable params: 2,097,665\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 0.7316 - accuracy: 0.5645 - val_loss: 0.5571 - val_accuracy: 0.7650\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 14s 137ms/step - loss: 0.6345 - accuracy: 0.6365 - val_loss: 0.4924 - val_accuracy: 0.8080\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 14s 139ms/step - loss: 0.5816 - accuracy: 0.6970 - val_loss: 0.4447 - val_accuracy: 0.8350\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 13s 133ms/step - loss: 0.5483 - accuracy: 0.7255 - val_loss: 0.4174 - val_accuracy: 0.8380\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.5013 - accuracy: 0.7630 - val_loss: 0.4010 - val_accuracy: 0.8330\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.5045 - accuracy: 0.7475 - val_loss: 0.3720 - val_accuracy: 0.8510\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.4864 - accuracy: 0.7635 - val_loss: 0.3579 - val_accuracy: 0.8460\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.4708 - accuracy: 0.7820 - val_loss: 0.3482 - val_accuracy: 0.8580\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.4696 - accuracy: 0.7695 - val_loss: 0.3494 - val_accuracy: 0.8550\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.4502 - accuracy: 0.7825 - val_loss: 0.3254 - val_accuracy: 0.8650\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 13s 135ms/step - loss: 0.4558 - accuracy: 0.7775 - val_loss: 0.3200 - val_accuracy: 0.8720\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.4299 - accuracy: 0.7985 - val_loss: 0.3124 - val_accuracy: 0.8690\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.4164 - accuracy: 0.8035 - val_loss: 0.3054 - val_accuracy: 0.8720\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.4243 - accuracy: 0.8060 - val_loss: 0.3023 - val_accuracy: 0.8670\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 13s 135ms/step - loss: 0.4116 - accuracy: 0.8125 - val_loss: 0.2994 - val_accuracy: 0.8650\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 13s 135ms/step - loss: 0.4103 - accuracy: 0.8160 - val_loss: 0.3017 - val_accuracy: 0.8700\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 13s 135ms/step - loss: 0.4072 - accuracy: 0.8055 - val_loss: 0.2935 - val_accuracy: 0.8660\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 13s 135ms/step - loss: 0.3924 - accuracy: 0.8165 - val_loss: 0.2934 - val_accuracy: 0.8710\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.4076 - accuracy: 0.8075 - val_loss: 0.2901 - val_accuracy: 0.8700\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 13s 135ms/step - loss: 0.3944 - accuracy: 0.8205 - val_loss: 0.2912 - val_accuracy: 0.8690\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 13s 135ms/step - loss: 0.4040 - accuracy: 0.8120 - val_loss: 0.2876 - val_accuracy: 0.8720\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.3899 - accuracy: 0.8180 - val_loss: 0.2815 - val_accuracy: 0.8730\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 13s 135ms/step - loss: 0.3766 - accuracy: 0.8290 - val_loss: 0.2862 - val_accuracy: 0.8750\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 14s 137ms/step - loss: 0.3886 - accuracy: 0.8210 - val_loss: 0.2765 - val_accuracy: 0.8680\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 14s 140ms/step - loss: 0.3846 - accuracy: 0.8200 - val_loss: 0.2763 - val_accuracy: 0.8700\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 13s 135ms/step - loss: 0.3848 - accuracy: 0.8230 - val_loss: 0.2805 - val_accuracy: 0.8710\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.3816 - accuracy: 0.8215 - val_loss: 0.2761 - val_accuracy: 0.8740\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.3785 - accuracy: 0.8270 - val_loss: 0.2759 - val_accuracy: 0.8750\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.3752 - accuracy: 0.8245 - val_loss: 0.2830 - val_accuracy: 0.8730\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 13s 135ms/step - loss: 0.3778 - accuracy: 0.8295 - val_loss: 0.2767 - val_accuracy: 0.8760\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "\n",
    "base_dir = '../data/cat_dog_small'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1/255,    \n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, \n",
    "    target_size=(150,150),\n",
    "    batch_size=20,\n",
    "    classes=['cats','dogs'],\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir, \n",
    "    target_size=(150,150),\n",
    "    batch_size=20,\n",
    "    classes=['cats','dogs'],\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# pretrained network\n",
    "model_base = VGG16(include_top=False, weights='imagenet', input_shape=(150,150,3))\n",
    "# model_base의 weight학습을 동결\n",
    "model_base.trainable=False\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# pretrained network를 우리의 모델 앞에 추가\n",
    "model.add(model_base)\n",
    "\n",
    "model.add(Flatten(input_shape=(4*4*512,)))\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(learning_rate=2e-5), loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_generator, steps_per_epoch=100, epochs=30,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alpha-virus",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 14s 137ms/step - loss: 0.4021 - accuracy: 0.8145 - val_loss: 0.2861 - val_accuracy: 0.8840\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.3277 - accuracy: 0.8455 - val_loss: 0.2364 - val_accuracy: 0.8940\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.3162 - accuracy: 0.8625 - val_loss: 0.2276 - val_accuracy: 0.9020\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.2989 - accuracy: 0.8730 - val_loss: 0.2219 - val_accuracy: 0.9040\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.2862 - accuracy: 0.8715 - val_loss: 0.2264 - val_accuracy: 0.8920\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 13s 135ms/step - loss: 0.2769 - accuracy: 0.8750 - val_loss: 0.2092 - val_accuracy: 0.9070\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.2635 - accuracy: 0.8820 - val_loss: 0.2389 - val_accuracy: 0.8950\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.2533 - accuracy: 0.8985 - val_loss: 0.2077 - val_accuracy: 0.9070\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 14s 142ms/step - loss: 0.2386 - accuracy: 0.8975 - val_loss: 0.2097 - val_accuracy: 0.9080\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.2303 - accuracy: 0.9080 - val_loss: 0.2021 - val_accuracy: 0.9160\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 14s 137ms/step - loss: 0.2285 - accuracy: 0.9040 - val_loss: 0.2088 - val_accuracy: 0.9120\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.2204 - accuracy: 0.9065 - val_loss: 0.1971 - val_accuracy: 0.9150\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.2064 - accuracy: 0.9105 - val_loss: 0.2062 - val_accuracy: 0.9130\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.2058 - accuracy: 0.9120 - val_loss: 0.2024 - val_accuracy: 0.9130\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.2047 - accuracy: 0.9110 - val_loss: 0.1923 - val_accuracy: 0.9170\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 14s 141ms/step - loss: 0.1918 - accuracy: 0.9205 - val_loss: 0.1996 - val_accuracy: 0.9090\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 14s 139ms/step - loss: 0.1872 - accuracy: 0.9285 - val_loss: 0.2026 - val_accuracy: 0.9050\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.1832 - accuracy: 0.9265 - val_loss: 0.2006 - val_accuracy: 0.9140\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.1848 - accuracy: 0.9350 - val_loss: 0.2436 - val_accuracy: 0.9010\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.1804 - accuracy: 0.9295 - val_loss: 0.1973 - val_accuracy: 0.9120\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.1767 - accuracy: 0.9195 - val_loss: 0.2323 - val_accuracy: 0.9040\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.1657 - accuracy: 0.9340 - val_loss: 0.2336 - val_accuracy: 0.9060\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.1623 - accuracy: 0.9335 - val_loss: 0.2072 - val_accuracy: 0.9190\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.1518 - accuracy: 0.9375 - val_loss: 0.2499 - val_accuracy: 0.9020\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.1446 - accuracy: 0.9430 - val_loss: 0.2105 - val_accuracy: 0.9170\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.1305 - accuracy: 0.9455 - val_loss: 0.2017 - val_accuracy: 0.9190\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.1423 - accuracy: 0.9450 - val_loss: 0.2304 - val_accuracy: 0.9180\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.1441 - accuracy: 0.9485 - val_loss: 0.2603 - val_accuracy: 0.9070\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.1191 - accuracy: 0.9490 - val_loss: 0.2333 - val_accuracy: 0.9200\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.1263 - accuracy: 0.9480 - val_loss: 0.2290 - val_accuracy: 0.9200\n"
     ]
    }
   ],
   "source": [
    "model_base.trainable=True\n",
    "\n",
    "# 상위 layer 동결해제\n",
    "for layer in model_base.layers:\n",
    "    if layer.name in ['block5_conv1','block5_conv2','block5_conv3']:\n",
    "        layer.trainable=True\n",
    "    else:\n",
    "        layer.trainable=False\n",
    "\n",
    "# 미세조정이므로 learning_rate를 더 작게 설정\n",
    "model.compile(optimizer=RMSprop(learning_rate=1e-5), loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_generator, steps_per_epoch=100, epochs=30,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env_tf2] *",
   "language": "python",
   "name": "conda-env-data_env_tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
