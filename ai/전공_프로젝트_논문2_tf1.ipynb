{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# train 폴더 변환\n",
    "path_dir = '../data/telemoji/pre/test/'\n",
    "file_list = os.listdir(path_dir)\n",
    "\n",
    "for idx, fname in enumerate(file_list):\n",
    "    # train 폴더 안의 폴더 경로\n",
    "    data = list()\n",
    "    temp_dir = os.path.join(path_dir, fname)\n",
    "    for iname in os.listdir(temp_dir):\n",
    "        img = plt.imread(os.path.join(temp_dir, iname))\n",
    "        data.append(np.concatenate((img.ravel(), [idx]), axis=0))\n",
    "    data = pd.DataFrame(data)\n",
    "    data.to_csv(f'../data/telemoji/pre/{fname}.csv', index=False)\n",
    "    del data\n",
    "    print(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder\n",
    "X = tf.placeholder(shape=[None, 150, 150, 3], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None, 7], dtype=tf.float32)\n",
    "\n",
    "### block 1 ###\n",
    "# filter 1\n",
    "W1 = tf.Variable(tf.random.normal([3, 3, 3, 64]))\n",
    "\n",
    "# convolution 1\n",
    "C1 = tf.nn.conv2d(X, W1, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C1 = tf.nn.relu(C1)\n",
    "\n",
    "# filter 2\n",
    "W2 = tf.Variable(tf.random.normal([3, 3, 64, 64]))\n",
    "\n",
    "# convolution 2\n",
    "C2 = tf.nn.conv2d(C1, W2, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C2 = tf.nn.relu(C2)\n",
    "\n",
    "# max pooling 1\n",
    "P1 = tf.nn.max_pool(C2, ksize=[1, 2, 2, 1], \n",
    "                    strides=[1, 2, 2, 1], padding='VALID')\n",
    "print(P1.shape)\n",
    "# > (?,75,75,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1 = tf.reshape(P1, [-1, 75*75*64])\n",
    "\n",
    "W18 = tf.Variable(tf.random.normal([75*75*64, 7]))\n",
    "b18 = tf.Variable(tf.random.normal([7]))\n",
    "\n",
    "# hypothesis\n",
    "logit = tf.matmul(P1, W18) + b18\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, \n",
    "                                                                 labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.AdamOptimizer(learning_rate=1e-2).minimize(loss)\n",
    "\n",
    "# sess\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### block 2 ###\n",
    "# filter 3\n",
    "W3 = tf.Variable(tf.random.normal([3, 3, 64, 128]))\n",
    "\n",
    "# convolution 3\n",
    "C3 = tf.nn.conv2d(P1, W3, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C3 = tf.nn.relu(C3)\n",
    "\n",
    "# filter 4\n",
    "W4 = tf.Variable(tf.random.normal([3, 3, 128, 128]))\n",
    "\n",
    "# convolution 4\n",
    "C4 = tf.nn.conv2d(C3, W4, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C4 = tf.nn.relu(C4)\n",
    "\n",
    "# filter 5\n",
    "W5 = tf.Variable(tf.random.normal([3, 3, 128, 128]))\n",
    "\n",
    "# convolution 5\n",
    "C5 = tf.nn.conv2d(C4, W5, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C5 = tf.nn.relu(C5)\n",
    "\n",
    "# pooling 2\n",
    "P2 = tf.nn.max_pool(C5, ksize=[1, 2, 2, 1], \n",
    "                    strides=[1, 2, 2, 1], padding='VALID')\n",
    "print(P2.shape)\n",
    "# > (?,37,37,128)\n",
    "\n",
    "# FC 1\n",
    "FC1 = tf.reshape(P2, [-1, 37*37*128])\n",
    "\n",
    "W_FC1 = tf.Variable(tf.random.normal([37*37*128, 256]))\n",
    "b_FC1 = tf.Variable(tf.random.normal([256]))\n",
    "FC1 = tf.nn.relu(tf.matmul(FC1, W_FC1) + b_FC1)\n",
    "\n",
    "print(FC1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### block 3 ###\n",
    "# filter 6\n",
    "W6 = tf.Variable(tf.random.normal([3, 3, 128, 256]))\n",
    "\n",
    "# convolution 6\n",
    "C6 = tf.nn.conv2d(P2, W6, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C6 = tf.nn.relu(C6)\n",
    "\n",
    "# filter 7\n",
    "W7 = tf.Variable(tf.random.normal([3, 3, 256, 256]))\n",
    "\n",
    "# convolution 7\n",
    "C7 = tf.nn.conv2d(C6, W7, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C7 = tf.nn.relu(C7)\n",
    "\n",
    "# filter 8\n",
    "W8 = tf.Variable(tf.random.normal([3, 3, 256, 256]))\n",
    "\n",
    "# convolution 8\n",
    "C8 = tf.nn.conv2d(C7, W8, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C8 = tf.nn.relu(C8)\n",
    "\n",
    "# filter 9\n",
    "W9 = tf.Variable(tf.random.normal([3, 3, 256, 256]))\n",
    "\n",
    "# convolution 9\n",
    "C9 = tf.nn.conv2d(C8, W9, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C9 = tf.nn.relu(C9)\n",
    "\n",
    "# pooling 3\n",
    "P3 = tf.nn.max_pool(C9, ksize=[1, 2, 2, 1], \n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "print(P3.shape)\n",
    "# > (?,19,19,256)\n",
    "\n",
    "# FC 2\n",
    "FC2 = tf.reshape(P3, [-1, 19*19*256])\n",
    "\n",
    "W_FC2 = tf.Variable(tf.random.normal([19*19*256, 256]))\n",
    "b_FC2 = tf.Variable(tf.random.normal([256]))\n",
    "FC2 = tf.nn.relu(tf.matmul(FC2, W_FC2) + b_FC2)\n",
    "print(FC2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### block 4 ###\n",
    "# filter 10\n",
    "W10 = tf.Variable(tf.random.normal([3, 3, 256, 256]))\n",
    "\n",
    "# convolution 10\n",
    "C10 = tf.nn.conv2d(P3, W10, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C10 = tf.nn.relu(C10)\n",
    "\n",
    "# filter 11\n",
    "W11 = tf.Variable(tf.random.normal([3, 3, 256, 256]))\n",
    "\n",
    "# convolution 11\n",
    "C11 = tf.nn.conv2d(C10, W11, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C11 = tf.nn.relu(C11)\n",
    "\n",
    "# filter 12\n",
    "W12 = tf.Variable(tf.random.normal([3, 3, 256, 256]))\n",
    "\n",
    "# convolution 12\n",
    "C12 = tf.nn.conv2d(C11, W12, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C12 = tf.nn.relu(C12)\n",
    "\n",
    "# filter 13\n",
    "W13 = tf.Variable(tf.random.normal([3, 3, 256, 256]))\n",
    "\n",
    "# convolution 13\n",
    "C13 = tf.nn.conv2d(C12, W13, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C13 = tf.nn.relu(C13)\n",
    "\n",
    "# pooling 4\n",
    "P4 = tf.nn.max_pool(C13, ksize=[1, 2, 2, 1], \n",
    "                    strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "print(P4.shape)\n",
    "# > (?,9,9,256)\n",
    "\n",
    "# FC 3\n",
    "FC3 = tf.reshape(P4, [-1, 9*9*256])\n",
    "\n",
    "W_FC3 = tf.Variable(tf.random.normal([9*9*256, 256]))\n",
    "b_FC3 = tf.Variable(tf.random.normal([256]))\n",
    "FC3 = tf.nn.relu(tf.matmul(FC3, W_FC3) + b_FC3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### block 5 ###\n",
    "# filter 14\n",
    "W14 = tf.Variable(tf.random.normal([3, 3, 256, 512]))\n",
    "\n",
    "# convolution 14\n",
    "C14 = tf.nn.conv2d(P4, W14, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C14 = tf.nn.relu(C14)\n",
    "\n",
    "# filter 15\n",
    "W15 = tf.Variable(tf.random.normal([3, 3, 512, 512]))\n",
    "\n",
    "# convolution 15\n",
    "C15 = tf.nn.conv2d(C14, W15, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C15 = tf.nn.relu(C15)\n",
    "\n",
    "# filter 16\n",
    "W16 = tf.Variable(tf.random.normal([3, 3, 512, 512]))\n",
    "\n",
    "# convolution 16\n",
    "C16 = tf.nn.conv2d(C15, W16, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C16 = tf.nn.relu(C16)\n",
    "\n",
    "# filter 17\n",
    "W17 = tf.Variable(tf.random.normal([3, 3, 512, 512]))\n",
    "\n",
    "# convolution 17\n",
    "C17 = tf.nn.conv2d(C16, W17, \n",
    "                  strides=[1,1,1,1], padding='SAME')\n",
    "C17 = tf.nn.relu(C17)\n",
    "\n",
    "# pooling 5\n",
    "P5 = tf.nn.max_pool(C17, ksize=[1, 2, 2, 1], \n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "print(P5.shape)\n",
    "# > (?,5,5,512)\n",
    "\n",
    "\n",
    "# flatten\n",
    "# DNN으로 들어가야할 것은 한개의 이미지에 대한 데이터\n",
    "FC4 = tf.reshape(P5, [-1, 5*5*512])\n",
    "\n",
    "W_FC4 = tf.Variable(tf.random.normal([5*5*512, 256]))\n",
    "b_FC4 = tf.Variable(tf.random.normal([256]))\n",
    "# FC4 = tf.nn.relu(tf.matmul(FC4, W_FC4) + b_FC4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat\n",
    "P6 = tf.concat([FC1, FC2, FC3, FC4], 1)\n",
    "print(P6.shape)\n",
    "\n",
    "W18 = tf.Variable(tf.random.normal([5*5*512, 7]))\n",
    "b18 = tf.Variable(tf.random.normal([7]))\n",
    "\n",
    "# hypothesis\n",
    "logit = tf.matmul(FC4, W18) + b18\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, \n",
    "                                                                 labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.AdamOptimizer(learning_rate=1e-2).minimize(loss)\n",
    "\n",
    "# sess\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ang_data = pd.read_csv('../data/telemoji/pre/angry.csv')\n",
    "dis_data = pd.read_csv('../data/telemoji/pre/disgust.csv')\n",
    "raw_data = pd.concat([ang_data, dis_data], axis=0)\n",
    "del ang_data\n",
    "del dis_data\n",
    "\n",
    "fea_data = pd.read_csv('../data/telemoji/pre/fear.csv')\n",
    "raw_data = pd.concat([raw_data, fea_data], axis=0)\n",
    "del fea_data\n",
    "\n",
    "hap_data = pd.read_csv('../data/telemoji/pre/happy.csv')\n",
    "raw_data = pd.concat([raw_data, hap_data], axis=0)\n",
    "del hap_data\n",
    "\n",
    "neu_data = pd.read_csv('../data/telemoji/pre/neutral.csv')\n",
    "raw_data = pd.concat([raw_data, neu_data], axis=0)\n",
    "del neu_data\n",
    "\n",
    "sad_data = pd.read_csv('../data/telemoji/pre/sad.csv')\n",
    "raw_data = pd.concat([raw_data, sad_data], axis=0)\n",
    "del sad_data\n",
    "\n",
    "sup_data = pd.read_csv('../data/telemoji/pre/surprise.csv')\n",
    "raw_data = pd.concat([raw_data, sup_data], axis=0)\n",
    "del sup_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data shuffle\n",
    "raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# data split\n",
    "x_data = raw_data.values[:,:-1].reshape(-1, 150, 150, 3)\n",
    "t_data = pd.get_dummies(raw_data.values[:,-1]).values\n",
    "\n",
    "x_data = x_data/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_epoch = 1000\n",
    "batch_size = 20\n",
    "\n",
    "for step in range(num_of_epoch):\n",
    "    total_batch = int(x_data.shape[0] / batch_size)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_x = x_data[i*batch_size:(i+1)*batch_size]\n",
    "        batch_t = t_data[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        _, loss_val = sess.run([train, loss], feed_dict={X: batch_x, \n",
    "                                                         T: batch_t})\n",
    "    if step % 100 == 0:\n",
    "        print(f'loss : {loss_val}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env] *",
   "language": "python",
   "name": "conda-env-data_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
